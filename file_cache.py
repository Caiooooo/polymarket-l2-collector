# cache the data until size limit or event ends
import os
import pandas as pd
from logger_config import setup_logger

logger = setup_logger('file_cache')

trade_limit = 100
book_limit = 100

# save book to data/1h/btc/orderbooks/1765436400down.parquet
# save trades to data/1h/btc/trades/1765436400down.parquet

# ä½¿ç”¨å­—å…¸å­˜å‚¨æ¯ä¸ªå¸‚åœºçš„ç¼“å­˜ï¼Œkey æ ¼å¼: "interval/coin/type/direction"
# ä¾‹å¦‚: "15m/btc/trades/up" æˆ– "1h/eth/orderbooks/down"
trades_cache_dict = {}
orderbook_cache_dict = {}


def optimize_data_for_parquet(data):
    """ä¼˜åŒ–æ•°æ®ä»¥æé«˜ parquet å‹ç¼©ç‡

    å°† price å’Œ size ä¹˜ä»¥ 100 å­˜å‚¨ä¸ºæ•´æ•°
    å°† price/size é”®åç²¾ç®€ä¸º p/s
    timestamp ä¿æŒä¸º long (int)
    """
    if not data:
        return data

    def convert_order_item(item):
        """è½¬æ¢è®¢å•é¡¹ä¸­çš„ price/size ä¸º p/s"""
        if not isinstance(item, dict):
            return item

        converted = {}
        for key, value in item.items():
            if key == 'price' and value is not None and not pd.isna(value):
                converted['p'] = int(float(value) * 100)
            elif key == 'size' and value is not None and not pd.isna(value):
                converted['s'] = int(float(value) * 100)
            else:
                converted[key] = value
        return converted

    optimized_data = []
    for record in data:
        optimized_record = {}

        for key, value in record.items():
            # å¤„ç† bids å’Œ asks åˆ—è¡¨
            if key in ['bids', 'asks'] and isinstance(value, list):
                optimized_record[key] = [
                    convert_order_item(item) for item in value]
            # å¤„ç†é¡¶å±‚çš„ price (è½¬ä¸º p)
            elif key == 'price' and value is not None and not pd.isna(value):
                optimized_record['p'] = int(float(value) * 100)
            # å¤„ç†é¡¶å±‚çš„ size (è½¬ä¸º s)
            elif key == 'size' and value is not None and not pd.isna(value):
                optimized_record['s'] = int(float(value) * 100)
            # timestamp ç¡®ä¿æ˜¯æ•´æ•°
            elif key == 'timestamp' and value is not None and not pd.isna(value):
                optimized_record[key] = int(value)
            # å…¶ä»–å­—æ®µä¿æŒä¸å˜
            else:
                optimized_record[key] = value

        optimized_data.append(optimized_record)

    return optimized_data


def restore_data_from_parquet(data):
    """ä» parquet æ¢å¤æ•°æ®

    å°† p/s æ¢å¤ä¸º price/size å¹¶é™¤ä»¥ 100 æ¢å¤ä¸ºæµ®ç‚¹æ•°
    """
    if not data:
        return data

    def restore_order_item(item):
        """æ¢å¤è®¢å•é¡¹ä¸­çš„ p/s ä¸º price/size"""
        if not isinstance(item, dict):
            return item

        restored = {}
        for key, value in item.items():
            if key == 'p' and value is not None and not pd.isna(value):
                restored['price'] = float(value) / 100
            elif key == 's' and value is not None and not pd.isna(value):
                restored['size'] = float(value) / 100
            # å‘åå…¼å®¹ï¼šå¤„ç†æ—§æ ¼å¼çš„ price/size
            elif key == 'price' and value is not None and not pd.isna(value):
                restored['price'] = float(value) / 100
            elif key == 'size' and value is not None and not pd.isna(value):
                restored['size'] = float(value) / 100
            else:
                restored[key] = value
        return restored

    restored_data = []
    for record in data:
        restored_record = {}

        for key, value in record.items():
            # å¤„ç† bids å’Œ asks åˆ—è¡¨
            if key in ['bids', 'asks'] and isinstance(value, list):
                restored_record[key] = [
                    restore_order_item(item) for item in value]
            # å¤„ç†é¡¶å±‚çš„ p (æ¢å¤ä¸º price)
            elif key == 'p' and value is not None and not pd.isna(value):
                restored_record['price'] = float(value) / 100
            # å¤„ç†é¡¶å±‚çš„ s (æ¢å¤ä¸º size)
            elif key == 's' and value is not None and not pd.isna(value):
                restored_record['size'] = float(value) / 100
            # å‘åå…¼å®¹ï¼šå¤„ç†æ—§æ ¼å¼çš„ price/size
            elif key == 'price' and value is not None and not pd.isna(value):
                restored_record['price'] = float(value) / 100
            elif key == 'size' and value is not None and not pd.isna(value):
                restored_record['size'] = float(value) / 100
            # å…¶ä»–å­—æ®µä¿æŒä¸å˜
            else:
                restored_record[key] = value

        restored_data.append(restored_record)

    return restored_data


def get_market_key(file_path):
    """ä»æ–‡ä»¶è·¯å¾„æå–å¸‚åœºæ ‡è¯†ç¬¦"""
    # file_path æ ¼å¼: data/15m/btc/trades/1765436400up.parquet
    parts = file_path.split("/")
    interval = parts[1]  # 15m æˆ– 1h
    coin = parts[2]      # btc, eth, sol, xrp
    data_type = parts[3]  # trades æˆ– orderbooks

    # æå–æ—¶é—´æˆ³å’Œæ–¹å‘
    filename = parts[4].split(".")[0]  # 1765436400up æˆ– 1765436400down
    direction = "up" if "up" in filename else "down"
    timestamp_str = filename.replace("up", "").replace("down", "")
    timestamp = int(timestamp_str)

    # åˆ¤æ–­æ–¹å‘

    market_key = f"{interval}/{coin}/{data_type}/{direction}"

    return market_key, timestamp


def save_trades(data, file_path):
    global trades_cache_dict
    # print(data, file_path)
    market_key, timestamp = get_market_key(file_path)

    # åˆå§‹åŒ–è¯¥å¸‚åœºçš„ç¼“å­˜ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if market_key not in trades_cache_dict:
        trades_cache_dict[market_key] = {
            'data': [],
            'timestamp': timestamp
        }

    cache_info = trades_cache_dict[market_key]

    # å¦‚æœæ˜¯æ–°çš„æ—¶é—´çª—å£ï¼Œä¿å­˜ä¹‹å‰çš„ç¼“å­˜å¹¶æ¸…ç©º
    if cache_info['timestamp'] != timestamp:
        print("new timestamp", timestamp)
        old_timestamp = cache_info['timestamp']
        if cache_info['data']:
            # æ„å»ºæ—§çš„æ–‡ä»¶è·¯å¾„
            old_file_path = file_path.replace(
                str(timestamp), str(old_timestamp))
            os.makedirs(os.path.dirname(old_file_path), exist_ok=True)

            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(old_file_path):
                try:
                    df = pd.read_parquet(old_file_path)
                    existing_data = restore_data_from_parquet(
                        df.to_dict('records'))
                except (FileNotFoundError, Exception):
                    existing_data = []

            # åˆå¹¶ç°æœ‰æ•°æ®å’Œç¼“å­˜æ•°æ®
            existing_data.extend(cache_info['data'])

            # ä¼˜åŒ–å¹¶ä¿å­˜åˆå¹¶åçš„æ•°æ®
            optimized_data = optimize_data_for_parquet(existing_data)
            df = pd.DataFrame(optimized_data)
            df.to_parquet(old_file_path, index=False,
                          engine='pyarrow', compression='zstd')
            logger.info(f"ğŸ’¾ äº¤æ˜“å·²ä¿å­˜: {old_file_path} ({len(existing_data)} æ¡)")

        # æ¸…ç©ºç¼“å­˜
        cache_info['data'] = []
        cache_info['timestamp'] = timestamp

    # è¿½åŠ æ–°æ•°æ®
    cache_info['data'].extend(data)

    # å¦‚æœè¾¾åˆ°ç¼“å­˜é™åˆ¶ï¼Œç«‹å³ä¿å­˜
    if len(cache_info['data']) >= trade_limit:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # è¯»å–ç°æœ‰æ•°æ®
        existing_data = []
        if os.path.exists(file_path):
            try:
                df = pd.read_parquet(file_path)
                existing_data = restore_data_from_parquet(
                    df.to_dict('records'))
            except (FileNotFoundError, Exception):
                existing_data = []

        # åˆå¹¶ç°æœ‰æ•°æ®å’Œç¼“å­˜æ•°æ®
        existing_data.extend(cache_info['data'])

        # ä¼˜åŒ–å¹¶ä¿å­˜åˆå¹¶åçš„æ•°æ®
        optimized_data = optimize_data_for_parquet(existing_data)
        df = pd.DataFrame(optimized_data)
        df.to_parquet(file_path, index=False,
                      engine='pyarrow', compression='zstd')
        # logger.info(f"ğŸ’¾ äº¤æ˜“å·²ä¿å­˜(è¾¾åˆ°é™åˆ¶): {file_path} ({len(existing_data)} æ¡)")
        # æ¸…ç©ºç¼“å­˜
        cache_info['data'] = []


def save_book(data, file_path):
    global orderbook_cache_dict

    market_key, timestamp = get_market_key(file_path)

    # åˆå§‹åŒ–è¯¥å¸‚åœºçš„ç¼“å­˜ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if market_key not in orderbook_cache_dict:
        orderbook_cache_dict[market_key] = {
            'data': [],
            'timestamp': timestamp
        }

    cache_info = orderbook_cache_dict[market_key]

    # å¦‚æœæ˜¯æ–°çš„æ—¶é—´çª—å£ï¼Œä¿å­˜ä¹‹å‰çš„ç¼“å­˜å¹¶æ¸…ç©º
    if cache_info['timestamp'] != timestamp:
        old_timestamp = cache_info['timestamp']
        if cache_info['data']:
            # æ„å»ºæ—§çš„æ–‡ä»¶è·¯å¾„
            old_file_path = file_path.replace(
                str(timestamp), str(old_timestamp))
            os.makedirs(os.path.dirname(old_file_path), exist_ok=True)

            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(old_file_path):
                try:
                    df = pd.read_parquet(old_file_path)
                    existing_data = restore_data_from_parquet(
                        df.to_dict('records'))
                except (FileNotFoundError, Exception):
                    existing_data = []

            # åˆå¹¶ç°æœ‰æ•°æ®å’Œç¼“å­˜æ•°æ®
            existing_data.extend(cache_info['data'])

            # ä¼˜åŒ–å¹¶ä¿å­˜åˆå¹¶åçš„æ•°æ®
            optimized_data = optimize_data_for_parquet(existing_data)
            df = pd.DataFrame(optimized_data)
            df.to_parquet(old_file_path, index=False,
                          engine='pyarrow', compression='zstd')
            logger.info(f"ğŸ’¾ è®¢å•ç°¿å·²ä¿å­˜: {old_file_path} ({len(existing_data)} æ¡)")

        # æ¸…ç©ºç¼“å­˜
        cache_info['data'] = []
        cache_info['timestamp'] = timestamp

    # è¿½åŠ æ–°æ•°æ®
    cache_info['data'].extend(data)

    # å¦‚æœè¾¾åˆ°ç¼“å­˜é™åˆ¶ï¼Œç«‹å³ä¿å­˜
    if len(cache_info['data']) >= book_limit:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # è¯»å–ç°æœ‰æ•°æ®
        existing_data = []
        if os.path.exists(file_path):
            try:
                df = pd.read_parquet(file_path)
                existing_data = restore_data_from_parquet(
                    df.to_dict('records'))
            except (FileNotFoundError, Exception):
                existing_data = []

        # åˆå¹¶ç°æœ‰æ•°æ®å’Œç¼“å­˜æ•°æ®
        existing_data.extend(cache_info['data'])

        # ä¼˜åŒ–å¹¶ä¿å­˜åˆå¹¶åçš„æ•°æ®
        optimized_data = optimize_data_for_parquet(existing_data)
        df = pd.DataFrame(optimized_data)
        df.to_parquet(file_path, index=False,
                      engine='pyarrow', compression='zstd')
        # logger.info(f"ğŸ’¾ è®¢å•ç°¿å·²ä¿å­˜(è¾¾åˆ°é™åˆ¶): {file_path} ({len(existing_data)} æ¡)")
        # æ¸…ç©ºç¼“å­˜
        cache_info['data'] = []
