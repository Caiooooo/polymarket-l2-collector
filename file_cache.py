# cache the data until size limit or event ends
import os
import json
from logger_config import setup_logger

logger = setup_logger('file_cache')

trade_limit = 100
book_limit = 100

# save book to data/1h/btc/orderbooks/1765436400down.json
# save trades to data/1h/btc/trades/1765436400down.json

# ä½¿ç”¨å­—å…¸å­˜å‚¨æ¯ä¸ªå¸‚åœºçš„ç¼“å­˜ï¼Œkey æ ¼å¼: "interval/coin/type/direction"
# ä¾‹å¦‚: "15m/btc/trades/up" æˆ– "1h/eth/orderbooks/down"
trades_cache_dict = {}
orderbook_cache_dict = {}


def get_market_key(file_path):
    """ä»æ–‡ä»¶è·¯å¾„æå–å¸‚åœºæ ‡è¯†ç¬¦"""
    # file_path æ ¼å¼: data/15m/btc/trades/1765436400up.json
    parts = file_path.split("/")
    interval = parts[1]  # 15m æˆ– 1h
    coin = parts[2]      # btc, eth, sol, xrp
    data_type = parts[3]  # trades æˆ– orderbooks

    # æå–æ—¶é—´æˆ³å’Œæ–¹å‘
    filename = parts[4].split(".")[0]  # 1765436400up æˆ– 1765436400down
    direction = "up" if "up" in filename else "down"
    timestamp_str = filename.replace("up", "").replace("down", "")
    timestamp = int(timestamp_str)

    # åˆ¤æ–­æ–¹å‘

    market_key = f"{interval}/{coin}/{data_type}/{direction}"

    return market_key, timestamp


def save_trades(data, file_path):
    global trades_cache_dict
    # print(data, file_path)
    market_key, timestamp = get_market_key(file_path)

    # åˆå§‹åŒ–è¯¥å¸‚åœºçš„ç¼“å­˜ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if market_key not in trades_cache_dict:
        trades_cache_dict[market_key] = {
            'data': [],
            'timestamp': timestamp
        }

    cache_info = trades_cache_dict[market_key]

    # å¦‚æœæ˜¯æ–°çš„æ—¶é—´çª—å£ï¼Œä¿å­˜ä¹‹å‰çš„ç¼“å­˜å¹¶æ¸…ç©º
    if cache_info['timestamp'] != timestamp:
        print("new timestamp", timestamp)
        old_timestamp = cache_info['timestamp']
        if cache_info['data']:
            # æ„å»ºæ—§çš„æ–‡ä»¶è·¯å¾„
            old_file_path = file_path.replace(
                str(timestamp), str(old_timestamp))
            os.makedirs(os.path.dirname(old_file_path), exist_ok=True)

            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(old_file_path):
                try:
                    with open(old_file_path, 'r') as f:
                        existing_data = json.load(f)
                except (json.JSONDecodeError, FileNotFoundError):
                    existing_data = []

            # åˆå¹¶ç°æœ‰æ•°æ®å’Œç¼“å­˜æ•°æ®
            existing_data.extend(cache_info['data'])

            # ä¿å­˜åˆå¹¶åçš„æ•°æ®
            with open(old_file_path, 'w') as f:
                json.dump(existing_data, f, indent=4)
            logger.info(f"ğŸ’¾ äº¤æ˜“å·²ä¿å­˜: {old_file_path} ({len(existing_data)} æ¡)")

        # æ¸…ç©ºç¼“å­˜
        cache_info['data'] = []
        cache_info['timestamp'] = timestamp

    # è¿½åŠ æ–°æ•°æ®
    cache_info['data'].extend(data)

    # å¦‚æœè¾¾åˆ°ç¼“å­˜é™åˆ¶ï¼Œç«‹å³ä¿å­˜
    if len(cache_info['data']) >= trade_limit:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # è¯»å–ç°æœ‰æ•°æ®
        existing_data = []
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r') as f:
                    existing_data = json.load(f)
            except (json.JSONDecodeError, FileNotFoundError):
                existing_data = []

        # åˆå¹¶ç°æœ‰æ•°æ®å’Œç¼“å­˜æ•°æ®
        existing_data.extend(cache_info['data'])

        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        with open(file_path, 'w') as f:
            json.dump(existing_data, f, indent=4)
        # logger.info(f"ğŸ’¾ äº¤æ˜“å·²ä¿å­˜(è¾¾åˆ°é™åˆ¶): {file_path} ({len(existing_data)} æ¡)")
        # æ¸…ç©ºç¼“å­˜
        cache_info['data'] = []


def save_book(data, file_path):
    global orderbook_cache_dict

    market_key, timestamp = get_market_key(file_path)

    # åˆå§‹åŒ–è¯¥å¸‚åœºçš„ç¼“å­˜ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if market_key not in orderbook_cache_dict:
        orderbook_cache_dict[market_key] = {
            'data': [],
            'timestamp': timestamp
        }

    cache_info = orderbook_cache_dict[market_key]

    # å¦‚æœæ˜¯æ–°çš„æ—¶é—´çª—å£ï¼Œä¿å­˜ä¹‹å‰çš„ç¼“å­˜å¹¶æ¸…ç©º
    if cache_info['timestamp'] != timestamp:
        old_timestamp = cache_info['timestamp']
        if cache_info['data']:
            # æ„å»ºæ—§çš„æ–‡ä»¶è·¯å¾„
            old_file_path = file_path.replace(
                str(timestamp), str(old_timestamp))
            os.makedirs(os.path.dirname(old_file_path), exist_ok=True)

            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(old_file_path):
                try:
                    with open(old_file_path, 'r') as f:
                        existing_data = json.load(f)
                except (json.JSONDecodeError, FileNotFoundError):
                    existing_data = []

            # åˆå¹¶ç°æœ‰æ•°æ®å’Œç¼“å­˜æ•°æ®
            existing_data.extend(cache_info['data'])

            # ä¿å­˜åˆå¹¶åçš„æ•°æ®
            with open(old_file_path, 'w') as f:
                json.dump(existing_data, f, indent=4)
            logger.info(f"ğŸ’¾ è®¢å•ç°¿å·²ä¿å­˜: {old_file_path} ({len(existing_data)} æ¡)")

        # æ¸…ç©ºç¼“å­˜
        cache_info['data'] = []
        cache_info['timestamp'] = timestamp

    # è¿½åŠ æ–°æ•°æ®
    cache_info['data'].extend(data)

    # å¦‚æœè¾¾åˆ°ç¼“å­˜é™åˆ¶ï¼Œç«‹å³ä¿å­˜
    if len(cache_info['data']) >= book_limit:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # è¯»å–ç°æœ‰æ•°æ®
        existing_data = []
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r') as f:
                    existing_data = json.load(f)
            except (json.JSONDecodeError, FileNotFoundError):
                existing_data = []

        # åˆå¹¶ç°æœ‰æ•°æ®å’Œç¼“å­˜æ•°æ®
        existing_data.extend(cache_info['data'])

        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        with open(file_path, 'w') as f:
            json.dump(existing_data, f, indent=4)
        # logger.info(f"ğŸ’¾ è®¢å•ç°¿å·²ä¿å­˜(è¾¾åˆ°é™åˆ¶): {file_path} ({len(existing_data)} æ¡)")
        # æ¸…ç©ºç¼“å­˜
        cache_info['data'] = []
